
<!-- This document was automatically generated with bibtex2html 1.96
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -dl -nodoc -nobibsource -nokeys -nokeywords -nofooter 2002.bib  -->




<p><a name="csdl2-01-11"></a>

Jitender Miglani.
 The design, implementation, and evaluation of INCA: an automated
  system for approval code allocation.
 M.S. Thesis CSDL-01-11, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, May 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2001/01-11/01-11.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

The ICS department of the University of Hawaii has faced problems
surrounding approval code distribution as its enrollment has increased. The
manual system for approval code allocation was time-consuming, ineffective
and inefficient. INCA is designed to automate the task of approval code
allocation, improve the quality of course approval decisions, and decrease
the administrative overhead involved in those decisions.

Based upon informal feedback from department administrators, it appears
that INCA reduces their overhead and makes their life easier. What are the
old problems that are solved by INCA? Does INCA introduce new kinds of
problems for the administrator? What about the students? Are they
completely satisfied with the system? In what ways does the system benefit
the department as a whole?

This thesis discusses the design, implementation and evaluation of INCA. It
evaluates INCA from the viewpoint of the administrator, the students, and
the department. An analysis of emails received at uhmics@hawaii.edu account
indicates that INCA reduces administrative overhead. The results of the
user survey show that three quarters of students believe INCA improved
their course approval predictability and course requirements
understandability. They prefer INCA to old method of requesting approval
codes by email. INCA database analysis provided course demand information
and student statistics useful for departments. This evaluation of INCA from
three different perspectives provides useful insights for future
improvement of INCA and for improving the student experience with academic
systems in general.

 
</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-01"></a>

Bill Giebink.
 Bringing the faulkes telescope to classrooms in hawaii.
 M.S. Thesis Proposal CSDL-02-01, Department of Information and
  Computer Sciences, University of Hawaii, Honolulu, Hawaii 96822, March 2002.
[&nbsp;<a href="http://koa.ifa.hawaii.edu/~giebink/thesis/proposal.htm">http</a>&nbsp;]
<blockquote><font size="-1">

The Faulkes Telescope (FT), currently under construction on the summit of
Haleakala, Maui, Hawaii, will provide data from celestial observations to
schools in the United Kingdom and Hawaii. This project, with its unique
goal of building a telescope to be used exclusively for educational
purposes, is a joint venture between groups in the United Kingdom and
Hawaii. Teachers and students will be able to download data that has been
collected by the telescope on a previous occasion or sign up to have the
telescope collect data at a specific time for them. Current plans call for
data from the telescope to be delivered to classrooms in the form of raw
data files and images from processed raw data files. In addition to sharing
use of the telescope, part of the agreement between the UK and Hawaii
groups provides for the UK group to share all software developed for the
project with the Hawaii group. However, though a system for transporting
images to schools is being developed for the UK side, at present there is
no corresponding system for Hawaii. Also, at this point neither the British
nor Hawaii sides have a definite system for storing and transporting raw
data files.

A first step, therefore, toward making the FT useful
for students and teachers in Hawaii is to develop a plan for a complete
system to archive and transport telescope data. It is anticipated that a
plan for this system will include: 1) a specification of the required
hardware components, 2) a description of how data will move in and out of
the system, 3) a definition of the data pathway within the system, and 4) a
description of the data storage requirements (i.e. database). The
development of each of the components of the system will consist of a
discussion of available options followed by a suggestion of the best choice
of action. Development of this system is anticipated to be the topic for a
directed reading/research project to be undertaken during spring,
2002. After the system has been clearly defined there are some additional
questions to be answered. Among the more interesting aspects is the
question of how to present data from the telescope in the most useful and
effective manner to teachers and students.
 
</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-02"></a>

Weifeng Miao.
 J2EEVAL: A method for performance analysis of enterprise javabean
  applications.
 M.S. Thesis CSDL-02-02, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, August 2002.
<blockquote><font size="-1">

J2EEval is a method for performance analysis of Enterprise JavaBean (EJB)
applications.  This thesis overviews the method and its application in
the context of a case study of the Inca Course approval system.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-03"></a>

Philip&nbsp;M. Johnson.
 Improving the dependability and predictability of jpl/mds software
  through low-overhead validation of software process and product metrics.
 Technical Report CSDL-02-03, Department of Information and
  Computer Sciences, University of Hawaii, Honolulu, Hawaii 96822, May 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-03/">http</a>&nbsp;]
<blockquote><font size="-1">
This white paper presents information regarding a proposed collaboration between
the Collaborative Software Development Laboratory, the Mission Data Systems group at Jet Propulsion Laboratory, and the Center for Software Engineering at University of Southern California. The proposed
collaboration would be funded through grants from the NSF/NASA Highly Dependable Computing and Communication Systems Research (HDCCSR) program.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-04"></a>

Joy&nbsp;M. Agustin, William&nbsp;M. Albritton, and Nolan&nbsp;Y. Kiddo.
 Virtual mall management software.
 Technical Report CSDL-02-04, Department of Information and
  Computer Sciences, University of Hawaii, Honolulu, Hawaii 96822, May 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-04/02-04.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Presents a business plan for commercialization of the Vendor Relationship Management (VRM) system.
</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-05"></a>

Philip&nbsp;M. Johnson.
 Supporting development of highly dependable software through
  continous, automated, in-process, and individualized software measurement
  validation.
 Technical Report CSDL-02-05, Department of Information and
  Computer Sciences, University of Hawaii, Honolulu, Hawaii 96822, July 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-05/02-05.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Highly dependable software is, by nature, predictable.  For example, one
can predict with confidence the circumstances under which the software will
work and the circumstances under which it will fail.  Empirically-based
approaches to creating predictable software are based on two assumptions:
(1) historical data can be used to develop and calibrate models that
generate empirical predictions, and (2) there exists relationships between
internal attributes of the software (i.e.  immediately measurable
process and product attributes such as size, effort, defects, complexity,
and so forth) and external attributes of the software (i.e. abstract
and/or non-immediately measurable attributes, such as `quality', the time
and circumstances of a specific component's failure in the field, and so
forth).  Software measurement validation is the process of
determining a predictive relationship between available internal attributes
and correspondingly useful external attributes and the conditions under
which this relationship holds.
&lt;p&gt;
This report proposes research whose general objective is to design, implement, and
validate software measures within a development infrastructure that
supports the development of highly dependable software systems. The
measures and infrastructure are designed to support dependable software
development in two ways: (1) They will support identification of modules at
risk for being fault-prone, enabling more efficient and effective
allocation of quality assurance resources, and (2) They will support
incremental software development through continuous monitoring,
notifications, and analyses.  Empirical assessment of these methods and
measures during use on the Mission Data System project at Jet Propulsion
Laboratory will advance the theory and practice of dependable computing and
software measurement validation and provide new insight into the
technological and methodological problems associated with the current state
of the art.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-08"></a>

Joy&nbsp;M. Agustin.
 Jblanket: Support for extreme coverage in java unit testing.
 Technical Report CSDL-02-08, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, December 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-08/02-08.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Unit testing is a tool commonly used to ensure reliability in software
development and can be applied to the software development process as soon
as core functionality of a program is implemented. In conventional unit
testing, to properly design unit tests programmers will need to have access
to specifications and source code. However, this is not possible in Extreme
Programming (XP), where tests are created before any feature of a system is
ever implemented. Obviously, XP's approach does not lead to improper
testing, but instead leads to a different approach for testing.  JBlanket
is a tool developed in the Collaborative Software Development Laboratory
(CSDL) at the University of Hawai'i (UH) that is meant to assist these
types of "unconventional" testing that calculates method-level coverage in
Java programs, a coarse enough granularity of test case coverage whereby
programmers will not only be able to ensure that all of their unit tests
pass, but will also be able to test all of their currently implemented
methods. Unit testing where 100 percent of all unit tests must pass that also
exercises 100 percent of all non-trivial remaining implemented methods is called
Extreme Coverage. This research will attempt to show that Extreme Coverage
is useful in developing quality software.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-09"></a>

Hongbing Kou and Xiangli Xu.
 Most active file measurement in Hackystat.
 Technical Report CSDL-02-09, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, December 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-09/02-09.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Hackystat, an automated metric collection and analysis tool, adopts the
"Most Active File" measurement in five-minute time chunks to represent the
developers' effort. This measurement is validated internally in this
report. The results show that big time chunk sizes are highly linear
regressive with the standard time chunk size (1 minute). The percentage of
missed effort to total effort is very low with five minutes chunk size. And
the relative ranking with respect to the effort of the active files is only
slightly changed.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-10"></a>

Christoph Aschwanden and Aaron Kagawa.
 Comparing personal project metrics to support process and product
  improvement.
 Technical Report CSDL-02-10, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, December 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-10/02-10.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

Writing high quality software with a minimum of effort is an important
thing to learn. Various personal metric collection processes exist, such as
PSP and Hackystat. However, using the personal metric collection processes
gives only a partial indication of how a programmer stands amongst his
peers.  Personal metrics vary greatly amongst programmers and it is not
always clear what is the "correct" way to develop software.  This paper
compares personal programming characteristics of students in a class
environment. Metrics, such as CK Metrics, have been analyzed and compared
against a set of similar students in an attempt to find the correct or
accepted value for these metrics.  It is our belief that programmers can
gain much, if not, more information from comparing their personal metrics
against other programmers. Preliminary results show that people with more
experience in programming produce different metrics than those with less.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-02-11"></a>

Cliff Tomosada and Burt Leung.
 Configuration management and Hackystat: Initial steps to relating
  organizational and individual development.
 Technical Report CSDL-02-11, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, December 2002.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2002/02-11/02-11.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

Hackystat is a software development metrics collection tool that focuses on
individual developers.  Hackystat is able to provide a developer with a
personal analysis of his or her unique processes.  Source code
configuration management (SCM) systems, on the other hand, are a means of
storage for source code in a development community and serve as controller
for what each individual may contribute to the community.  We created a
Hackystat sensor for CVS (an SCM system) in the hopes of bridging the gap
between these two very different, yet related software applications.  It
was our hope to use the data we collected to address the issue of
development conflicts that often arise in organizational development
environments.  We found, however, that neither application, Hackystat or
CVS, could be easily reconfigured to our needs.

</font></blockquote>
<p>
</p>