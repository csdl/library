
<!-- This document was automatically generated with bibtex2html 1.96
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -dl -nodoc -nobibsource -nokeys -nokeywords -nofooter 2006.bib  -->




<p><a name="csdl2-06-01"></a>

Lutz Prechelt, Sebastian Jekutsch, and Philip&nbsp;M. Johnson.
 Actual process: A research program.
 In <em>Submitted to the 2006 Workshop on Software Process</em>, May
  2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2006/06-01/06-01.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

Most process research relies heavily on the use of terms and concepts whose
validity depends on a variety of assumptions to be met. As it is difficult
to guarantee that they are met, such work continually runs the risk of
being invalid. We propose a different and complementary approach to
understanding process: Perform all description bottom-up and based on hard
data alone. We call the approach actual process and the data actual
events. Actual events can be measured automatically. This paper describes
what has been done in this area already and what are the core problems to
be solved in the future.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-06-02"></a>

Hongbing Kou and Philip&nbsp;M. Johnson.
 Automated recognition of low-level process: A pilot validation study
  of Zorro for test-driven development.
 In <em>Proceedings of the 2006 International Workshop on Software
  Process</em>, Shanghai, China, May 2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2006/06-02/06-02.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

Zorro is a system designed to automatically determine whether a developer
is complying with the Test-Driven Development (TDD) process.  Automated
recognition of TDD could benefit the software engineering community in a
variety of ways, from pedagogical aids to support the learning of
test-driven design, to support for more rigorous empirical studies on the
effectiveness of TDD in practice.  This paper presents the Zorro system and
the results of a pilot validation study, which shows that Zorro was able to
recognize test-driven design episodes correctly 89% of the time. The
results also indicate ways to improve Zorro's classification accuracy
further, and provide evidence for the effectiveness of this approach to
low-level software process recognition.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-06-05"></a>

Qin Zhang.
 <em>Improving Software Development Process and Product Management
  with Software Project Telemetry</em>.
 Ph.D. thesis, University of Hawaii, Department of Information and
  Computer Sciences, December 2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2006/06-05/06-05.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

Software development is slow, expensive and error prone, often resulting in
products with a large number of defects which cause serious problems in
usability, reliability, and performance. To combat this problem, software
measurement provides a systematic and empirically-guided approach to
control and improve software development processes and final
products. However, due to the high cost associated with &ldquo;metrics
collection&rdquo; and difficulties in &ldquo;metrics decision-making,&rdquo; measurement
is not widely adopted by software organizations.

This dissertation proposes a novel metrics-based program called &ldquo;software
project telemetry&rdquo; to address the problems. It uses software sensors to
collect metrics automatically and unobtrusively. It employs a
domain-specific language to represent telemetry trends in software product
and process metrics. Project management and process improvement decisions
are made by detecting changes in telemetry trends and comparing trends
between different periods of the same project. Software project telemetry
avoids many problems inherent in traditional metrics models, such as the
need to accumulate a historical project database and ensure that the
historical data remain comparable to current and future projects.

The claim of this dissertation is that software project telemetry provides
an effective approach to (1) automated metrics collection and analysis, and
(2) in-process, empirically-guided software development process problem
detection and diagnosis. Two empirical studies were carried out to evaluate
the claim: one in software engineering classes, and the other in the
Collaborative Software Development Lab. The results suggested that software
project telemetry had acceptably-low metrics collection and analysis
overhead, and that it provided decision-making value at least in the
exploratory context of the two studies.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-06-08"></a>

Lorin Hochstein, Taiga Nakamura, Victor&nbsp;R. Basili, Sima Asgari, Marvin&nbsp;V.
  Zelkowitz, Jeffrey&nbsp;K. Hollingsworth, Forrest Shull, Jeffrey Carver, Martin
  Voelp, Nico Zazworka, and Philip&nbsp;M. Johnson.
 Experiments to understand HPC time to development.
 <em>CTWatch Quarterly</em>, November 2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2006/06-08/06-08.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

In order to understand how high performance computing (HPC) programs are
developed, a series of experiments, using students in graduate level HPC
classes, have been conducted at many universities in the US. In this paper
we discuss the general process of conducting those experiments, give some
of the early results of those experiments, and describe a web-based process
we are developing that will allow us to run additional experiments at other
universities and laboratories that will be easier to conduct and generate
results that more accurately reflect the process of building HPC programs.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-06-09"></a>

Takuya Yamashita.
 Evaluation of Jupiter: A lightweight code review framework.
 M.S. Thesis CSDL-06-09, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, December 2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2006/06-09/06-09.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

Software engineers generally agree that code reviews reduce development
costs and improve software quality by finding defects in the early stages
of software development. In addition, code review software tools help the
code review process by providing a more efficient means of collecting and
analyzing code review data. On the other hand, software organizations that
conduct code reviews often do not utilize these review tools. Instead, most
organizations simply use paper or text editors to support their code review
processes. Using paper or a text editor is potentially less useful than
using a review tool for collecting and analyzing code review data.

In this research, I attempted to address the problems of previous code
review tools by creating a lightweight and flexible review tool. This
review tool that I have developed, called "Jupiter", is an Eclipse IDE
Plug-In. I believe the Jupiter Code Review Tool is more efficient at
collecting and analyzing code review data than the text-based approaches.

To investigate this hypothesis, I have constructed a methodology to compare
the Jupiter Review Tool to the text-based review approaches. I carried out
a case study using both approaches in a software engineering course with 19
students.

The results provide some supporting evidence that Jupiter is more useful
and more usable than the text-based code review, requires less overhead
than the text-based review, and appears to support long-term adoption.

The major contributions of this research are the Jupiter design philosophy,
the Jupiter Code Review Tool, and the insights from the case study
comparing the text-based review to the Jupiter-based review.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-06-12"></a>

Hongbing Kou.
 Automated inference of software development behaviors: Design,
  implementation and validation of Zorro for test-driven development.
 Ph.D. Thesis Proposal CSDL-06-12, Department of Information and
  Computer Sciences, University of Hawaii, Honolulu, Hawaii 96822, November
  2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2006/06-12/06-12.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">

In my dissertation research, I propose to develop a systematic approach to automatically
inferring software development behaviors using a technique I have developed called Software Development
Stream Analysis (SDSA). Software Development Stream Analysis is a generic framework
for inferring low-level software development behaviors. Zorro is an implementation of SDSA
for Test-Driven Development (TDD). In addition, I designed a series of validation studies to test
the SDSA framework by evaluating Zorro with respect to its capabilities to infer TDD development
behaviors. An early pilot validation study found that Zorro works very well in practice, with Zorro
recognizing the software development episodes of TDD with 88.4% accuracy. After this pilot
study, I improved Zorro system's inferencing rules and evaluation mechanism as part of my collaborative
research with Software Engineering Group at the National Research Council of Canada
(NRC-CNRC). I am planning to conduct two more extended validation studies of Zorro in academic
and industrial settings for Fall 2006 and Spring 2007.

</font></blockquote>
<p>
</p>

<p><a name="csdl2-07-02"></a>

Philip&nbsp;M. Johnson.
 Results from the 2006 classroom evaluation of Hackystat-UH.
 Technical Report CSDL-07-02, Department of Information and Computer
  Sciences, University of Hawaii, Honolulu, Hawaii 96822, December 2006.
[&nbsp;<a href="http://csdl.ics.hawaii.edu/techreports/2007/07-02/07-02.html">.html</a>&nbsp;]
<blockquote><font size="-1">

This report presents the results from a classroom evaluation of Hackystat
by ICS 413 and ICS 613 students at the end of Fall, 2006.  The students had
used Hackystat-UH for approximately six weeks at the time of the
evaluation.  The survey requests their feedback regarding the installation,
configuration, overhead of use, usability, utility, and future use of the
Hackystat-UH configuration. This classroom evaluation is a semi-replication
of an evaluation performed on Hackystat by ICS 413 and 613 students at the
end of Fall, 2003, which is reported in "Results from the 2003 Classroom
Evaluation of Hackystat-UH".  As the Hackystat system has changed
significantly since 2003, some of the evaluation questions were changed.
The data from this evaluation, in combination with the data from the 2003
evaluation, provide an interesting perspective on the past, present, and
possible future of Hackystat.  Hackystat has increased significantly in
functionality since 2003, which has enabled the 2006 usage to more closely
reflect industrial application, and which has resulted in significantly
less overhead with respect to client-side installation. On the other hand,
results appear to indicate that this increase in functionality has resulted
in a decrease in the usability and utility of the system, due to
inadequacies in the server-side user interface.  Based upon the data, the
report proposes a set of user interface enhancements to address the
problems raised by the students, including Ajax-based menus and parameters,
workflow based organization of the user interface, real-time display for
ongoing project monitoring, annotations, and simplified data exploration
facilities.

</font></blockquote>
<p>
</p>